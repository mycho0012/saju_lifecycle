import yaml
import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
from langchain_teddynote.prompts import load_prompt
from lifecycle_functions_v1 import find_first_ipchun_revised, get_ipchun_opposite, get_ipchun_same, get_saju, get_solar_term
from lifecycle_functions_v1 import calculate_lifecycle, create_enhanced_3d_lifecycle_chart
from lifecycle_functions_v1 import load_saju_data
from dotenv import load_dotenv
from datetime import datetime
import os

# API KEY ì •ë³´ ë¡œë“œ
load_dotenv()

# ì „ì—­ ë³€ìˆ˜ë¡œ ì‚¬ì£¼ ë°ì´í„° ë¡œë“œ (ì„±ëŠ¥ ìµœì í™”)
SAJU_DATA = load_saju_data('saju_cal.csv')

def format_lifecycle(lifecycle_data):
    formatted = ""
    for i, (term, year, age) in enumerate(lifecycle_data):
        if i % 4 == 0 and i != 0:
            formatted += "\n"
        formatted += f"{term}({year:.1f}ë…„, {age}ì„¸) | "
    return formatted.strip()

def parse_date(input_str):
    if len(input_str) == 12 and input_str.isdigit():
        return input_str
    return None

def create_chain(model_choice):
    prompt = load_prompt("saju_prompt_general.yaml", encoding="utf-8")
    
    if model_choice == "OpenAI GPT":
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
    else:  # Google Gemini
        llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0)
    
    output_parser = StrOutputParser()
    chain = prompt | llm | output_parser
    return chain

def reset_session():
    for key in list(st.session_state.keys()):
        del st.session_state[key]

st.title("AI LIFE CYCLE ê¸¸ì¡ì´ ğŸ’¬")

with st.sidebar:
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        reset_session()
        st.rerun()  # ì—¬ê¸°ë¥¼ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤

    st.session_state.model_choice = st.selectbox(
        "AI ëª¨ë¸ì„ ì„ íƒí•´ ì£¼ì„¸ìš”", ("OpenAI GPT", "Google Gemini"), index=0
    )

if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'analyzed' not in st.session_state:
    st.session_state.analyzed = False

if not st.session_state.analyzed:
    gender = st.radio("ì„±ë³„ì„ ì„ íƒí•˜ì„¸ìš”:", ("ë‚¨ì„±", "ì—¬ì„±"))
    birth_input = st.text_input("ìƒë…„ì›”ì¼ì‹œë¥¼ ì…ë ¥í•˜ì„¸ìš” (YYYYMMDDHHMMM í˜•ì‹):")

    if st.button("ë¶„ì„ ì‹œì‘"):
        if gender and birth_input:
            parsed_date = parse_date(birth_input)
            if parsed_date:
                try:
                    birth_year = int(parsed_date[:4])
                    current_year = datetime.now().year

                    saju_result = get_saju(parsed_date, SAJU_DATA)
                    st.write(f"ë‹¹ì‹ ì˜ ì‚¬ì£¼íŒ”ì: {saju_result}")

                    ipchun_same = get_ipchun_same(saju_result)
                    ipchun_opposite = get_ipchun_opposite(saju_result)
                    month_ground = saju_result.split(',')[0].split()[1][1]
                    solar_term = get_solar_term(month_ground)

                    analysis_result = f"""
                    ì…ì¶˜ì  ì •ë³´:
                    1. ì¼ê°„ì›”ì§€ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œ ì…ì¶˜ì : {ipchun_same}ë…„
                       í•´ë‹¹ ì ˆê¸°: {solar_term}
                    2. ì›”ì§€ì˜ ì •ë°˜ëŒ€ ì§€ì§€ë¥¼ ì‚¬ìš©í•œ ì…ì¶˜ì : {ipchun_opposite}ë…„
                       í•´ë‹¹ ì ˆê¸°: {get_solar_term(ipchun_opposite[1])}
                    """

                    first_ipchun_same = find_first_ipchun_revised(birth_year, ipchun_same, SAJU_DATA)
                    first_ipchun_opposite = find_first_ipchun_revised(birth_year, ipchun_opposite, SAJU_DATA)

                    if first_ipchun_same:
                        analysis_result += f"\në‹¹ì‹ ì˜ ìƒì•  ì²« ìˆœë°©í–¥ ì…ì¶˜ì : ì…ì¶˜({first_ipchun_same[0]}, {first_ipchun_same[1]}ì„¸)"
                    if first_ipchun_opposite:
                        analysis_result += f"\në‹¹ì‹ ì˜ ìƒì•  ì²« ì—­ë°©í–¥ ì…ì¶˜ì : ì…ì¶˜({first_ipchun_opposite[0]}, {first_ipchun_opposite[1]}ì„¸)"

                    lifecycle_forward = calculate_lifecycle(ipchun_same, birth_year)
                    lifecycle_backward = calculate_lifecycle(ipchun_opposite, birth_year)

                    if lifecycle_forward:
                        analysis_result += "\n\nìˆœë°©í–¥ ì…ì¶˜ì ìœ¼ë¡œ ê³„ì‚°í•œ 60ë…„ ìƒì• ì£¼ê¸°:\n"
                        analysis_result += format_lifecycle(lifecycle_forward)
                    if lifecycle_backward:
                        analysis_result += "\n\nì—­ë°©í–¥ ì…ì¶˜ì ìœ¼ë¡œ ê³„ì‚°í•œ 60ë…„ ìƒì• ì£¼ê¸°:\n"
                        analysis_result += format_lifecycle(lifecycle_backward)

                    st.write(analysis_result)

                    st.subheader("ìƒì• ì£¼ê¸° 3D ì°¨íŠ¸")
                    
                    st.subheader("ìˆœë°©í–¥ ìƒì• ì£¼ê¸° 3D ì°¨íŠ¸")
                    fig_forward = create_enhanced_3d_lifecycle_chart(lifecycle_forward, birth_year)
                    st.plotly_chart(fig_forward, use_container_width=True)

                    st.subheader("ì—­ë°©í–¥ ìƒì• ì£¼ê¸° 3D ì°¨íŠ¸")
                    fig_backward = create_enhanced_3d_lifecycle_chart(lifecycle_backward, birth_year)
                    st.plotly_chart(fig_backward, use_container_width=True)

                    chain = create_chain(st.session_state.model_choice)
                    
                    lifecycle_str = format_lifecycle(lifecycle_backward)

                    initial_response = chain.invoke({
                        "saju": saju_result,
                        "lifecycle": lifecycle_str,
                        "birth_year": str(birth_year),
                        "current_year": str(current_year),
                        "gender": gender,
                        "question": "ì „ì²´ì ì¸ ì‚¬ì£¼ í•´ì„ê³¼ 24ì ˆê¸°ì— ê¸°ë°˜í•œ 60ë…„ ì¸ìƒì˜ ìƒì• ì£¼ê¸°ë¥¼ ìì„¸í•˜ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”."
                    })

                    st.subheader("AIì˜ ì‚¬ì£¼ í•´ì„ ë° ì¡°ì–¸")
                    st.write(initial_response)

                    st.session_state.analyzed = True
                    st.session_state.saju_result = saju_result
                    st.session_state.lifecycle_str = lifecycle_str
                    st.session_state.birth_year = birth_year
                    st.session_state.gender = gender
                    st.session_state.messages.append({"role": "assistant", "content": initial_response})

                except Exception as e:
                    st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            else:
                st.error("ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ìƒë…„ì›”ì¼ì‹œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            st.error("ì„±ë³„ê³¼ ìƒë…„ì›”ì¼ì‹œë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

else:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

if st.session_state.analyzed:
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        full_prompt = f"""
        ë‹¹ì‹ ì€ 30ë…„ ê²½ë ¥ì˜ ì‚¬ì£¼íŒ”ì í†µë³€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

        ì‚¬ìš©ìì˜ ì‚¬ì£¼íŒ”ì: {st.session_state.saju_result}
        ìƒë…„: {st.session_state.birth_year}
        ì„±ë³„: {st.session_state.gender}
        
        ì‚¬ìš©ì ì§ˆë¬¸: {prompt}
        
        ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
        1. ì‚¬ì£¼íŒ”ìì˜ ìŒì–‘ì˜¤í–‰ ê· í˜•ì„ ê³ ë ¤í•˜ì—¬ í•´ì„í•´ì£¼ì„¸ìš”.
        2. ì‚¬ìš©ìì˜ ì„±ë³„ê³¼ ë‚˜ì´ì— ë§ëŠ” ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        3. í˜„ëŒ€ì  ë§¥ë½ì—ì„œ ì‹¤ìš©ì ì¸ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”.
        4. ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.
        5. í•„ìš”í•˜ë‹¤ë©´ ìƒì• ì£¼ê¸° ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ì‚¬ì£¼ì™€ ìš´ì„¸ëŠ” ì ˆëŒ€ì ì¸ ê²ƒì´ ì•„ë‹ˆë¼ ì°¸ê³ ì‚¬í•­ì„ì„ ì–¸ê¸‰í•˜ê³ , ê°œì¸ì˜ ë…¸ë ¥ê³¼ ì„ íƒì´ ì¤‘ìš”í•¨ì„ ê°•ì¡°í•´ì£¼ì„¸ìš”.
        """
        
        if st.session_state.model_choice == "OpenAI GPT":
            llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
        else:  # Google Gemini
            llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0)
        
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = llm.predict(full_prompt)
            message_placeholder.markdown(full_response)
        st.session_state.messages.append({"role": "assistant", "content": full_response})
